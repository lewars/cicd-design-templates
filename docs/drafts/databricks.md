# Design Document: Databricks DataOps Platform

## 1. Overview

The primary objective of this platform is to automate the deployment of Databricks resources while strictly protecting the integrity of **Staging** and **Production** environments. By employing a **Colocated DAB Pattern**, we solve the critical issue of resource duplicationâ€”preventing scenarios where dashboards or jobs with identical names are accidentally published or overwritten. This system utilizes **GitHub Actions** to orchestrate a robust CI/CD pipeline, ensuring that every change is validated, versioned, and promoted through a structured lifecycle.

## 2. Environment Strategy & Isolation

The platform utilizes a three-tier architecture to isolate development from business-critical assets:

* **Sandbox**: Personal development areas where unique namespaces are generated per user and branch to prevent collisions.
* **Staging**: The integration layer where code is automatically validated against a production-like environment.
* **Production**: The restricted "Source of Truth," accessible only through automated service principals.

> **Note**: While the standard flow follows a structured promotion, an **accelerated release path** exists. This "escape hatch" allows developers to bypass the formal release-please PR by using a chatbot command directly within their feature PR. Detailed logic for this is provided in Section 6.2.

---

## 3. CI/CD & The "Fast-Track" Release

This section outlines the lifecycle of a code change as it moves through the automated pipeline. The platform employs a dual-path deployment strategy: a standard, versioned release cycle for planned updates and a "Fast-Track" ChatOps path for emergency hotfixes and urgent deliveries.

### 3.1 Workflow Visualization

```mermaid
graph TD
Â  Â  A[Feature Branch / PR] -->|Push| B[Lint & Unit Tests]
Â  Â  B -->|Merge to Main| C[Deploy to Staging]
Â  Â  C -->|Auto-Tests| D{Release Logic}
Â  Â  D -->|Release-Please PR| E[Prod Deployment]
Â  Â  A -->|ChatOps /release| E[Prod Deployment]
Â  Â  E -->|Success| F[Teams Notification]


```

### 3.2 The Deployment Workflow

1. **Continuous Integration (CI)**: Pushes to any feature branch trigger automated linting and unit tests to ensure code quality before reaching the peer review stage.
2. **Staging Promotion**: Upon merging a Pull Request into the `main` branch, the bundle is automatically deployed to the Staging environment.
3. **Integration Testing**: Automated resource validation tests run against the Staging environment to confirm that the Databricks Jobs, DLT pipelines, and Dashboards function correctly in a production-like setting.
4. **Version Management**: The `release-please` action monitors the `main` branch, automatically aggregating commits to create a Release PR that handles Semantic Versioning (SemVer) and changelog updates.
5. **Production Release**: Merging the Release PR triggers the production workflow, deploying assets to the Production environment using a Service Principal identity for maximum security.
6. **Fast-Track "Escape Hatch"**: Developers can bypass the standard release cycle by commenting `/release` on an open PR. This triggers an immediate merge to `main` and initiates the Production deployment pipeline, providing an accelerated path for urgent changes.

### 3.3 Automated Versioning (SemVer)

We use **Release Please** to calculate version bumps based on Conventional Commits:

* **`fix:`** â” Patch (1.0.1)
* **`feat:`** â” Minor (1.1.0)
* **`feat!:`** â” Major (2.0.0)

**ChatOps Feedback Loop:**

* **Start**: "ğŸš€ Fast-Track Release Initialized. Iâ€™m merging this feature and triggering the production release now."
* **Complete**: "âœ… Release Complete. The Release PR has been merged, and the Production deployment is underway."
* **Failure**: "âŒ Fast-Track Release Failed. I encountered an error... Please check GitHub Actions logs."

---

## 4. Standardized Project Blueprint

This layout is the mandatory standard for all current and future Databricks projects to ensure cross-team compatibility and ease of maintenance.

```text
.
â”œâ”€â”€ databricks.ymlÂ  Â  Â  Â  Â  Â  Â  # Main coordinator (The "Skeleton")
â”œâ”€â”€ Taskfile.ymlÂ  Â  Â  Â  Â  Â  Â  Â  # Command abstraction layer
â”œâ”€â”€ .github/
â”‚Â  Â â””â”€â”€ workflows/Â  Â  Â  Â  Â  Â  Â  # CI/CD Pipeline definitions
â”œâ”€â”€ targets/Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Environment-specific overrides
â”‚Â  Â â”œâ”€â”€ sandbox.ymlÂ  Â  Â  Â  Â  Â  Â # Personal dev settings
â”‚Â  Â â”œâ”€â”€ staging.ymlÂ  Â  Â  Â  Â  Â  Â # Integration settings
â”‚Â  Â â””â”€â”€ prod.ymlÂ  Â  Â  Â  Â  Â  Â  Â  # Production Service Principal settings
â”œâ”€â”€ resources/Â  Â  Â  Â  Â  Â  Â  Â  Â  # Shared resource definitions
â”‚Â  Â â”œâ”€â”€ jobs.ymlÂ  Â  Â  Â  Â  Â  Â  Â  # Workflow definitions
â”‚Â  Â â”œâ”€â”€ pipelines.ymlÂ  Â  Â  Â  Â  Â # DLT definitions
â”‚Â  Â â””â”€â”€ dashboards.ymlÂ  Â  Â  Â  Â  # AI/BI Dashboard definitions
â”œâ”€â”€ src/Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Code (Notebooks, Python, SQL)
â”œâ”€â”€ scripts/Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Automation (db_setup.sh, db_notify.sh)
â”œâ”€â”€ tests/Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Pytest validation suites
â”œâ”€â”€ requirements-dev.txtÂ  Â  Â  Â  # Local dev dependencies
â””â”€â”€ .gitignoreÂ  Â  Â  Â  Â  Â  Â  Â  Â  # Excludes .bundle/, secrets, and caches


```

---

## 5. Configuration & Automation Components

### 5.1 The Coordinator (`databricks.yml`)

The Databricks Asset Bundle (DAB) is a declarative framework that allows you to define your Databricks resources as code. The `databricks.yml` file acts as the primary "Skeleton," importing resources and defining global variables that the Databricks CLI uses to assemble the deployment package.

```yaml
bundle:
Â  name: marketing_analytics

include:
Â  - targets/*.yml
Â  - resources/*.yml


```

### 5.2 Target Overrides (`targets/`)

Targets provide environment-specific configurations. The CLI merges the base bundle with the specific target logic during deployment.

* **Sandbox**: For local iteration. Unique `root_path` prevents overwriting others' work.
* **Staging**: For final integration testing before production.
* **Production**: The locked environment using **Service Principal** identities.

**Variables and CLI Arguments:**
Variables allow for dynamic configuration. For example, in `sandbox.yml`, we use:

* `${workspace.current_user.short_name}`: To identify the developer.
* `${bundle.git_branch}`: To isolate different features.

You can override these via the CLI:
`databricks bundle deploy -t sandbox --var "catalog_name=experimental_catalog"`

### 5.3 Resource Definitions (`resources/`)

Files in this directory define the *what* of your project . By modularizing these, you can manage complex workflows in smaller, readable chunks.

The `resources/` directory defines the high-level Databricks objects (Jobs, DLT Pipelines, Dashboards) that the bundle will create and manage. By separating these into logical files, we maintain a clean and scalable configuration that can be easily reviewed during the PR process.

#### 5.3.1 Resource Categorization

Resources are split by type to prevent a single, unmanageable YAML file:

* **`dashboards.yml`**: Defines AI/BI dashboards, including layout, widgets, and data sources.
* **`jobs.yml`**: Configures multi-task workflows, including trigger schedules (cron), email notifications on failure, and cluster requirements.
* **`pipelines.yml`**: Specific to Delta Live Tables (DLT), defining materialization logic and data quality constraints (Expectations).

#### 5.3.2 Resource Overrides & Shared Logic

Because the `databricks.yml` coordinates these files, you can define a "base" resource and then apply environment-specific tweaks in your targets. For example, a job might run on a small shared cluster in **Staging** but move to a high-concurrency SQL Warehouse in **Production**.

#### 5.3.3 Implementation Example: `resources/jobs.yml`

```yaml
resources:
Â  jobs:
Â  Â  daily_ingestion_job:
Â  Â  Â  name: "[${bundle.target}] Daily Sales Ingestion"
Â  Â  Â  tasks:
Â  Â  Â  Â  - task_key: refresh_tables
Â  Â  Â  Â  Â  notebook_task:
Â  Â  Â  Â  Â  Â  notebook_path: ../src/notebooks/ingest_sales.py
Â  Â  Â  job_clusters:
Â  Â  Â  Â  - job_cluster_key: default_cluster
Â  Â  Â  Â  Â  new_cluster:
Â  Â  Â  Â  Â  Â  spark_version: "14.3.x-scala2.12"
Â  Â  Â  Â  Â  Â  node_type_id: "Standard_DS3_v2"
Â  Â  Â  Â  Â  Â  num_workers: 2


```

---

## 6. The Command Interface (`Taskfile.yml`)

The Taskfile acts as a human-readable wrapper for complex CLI commands.

```yaml
version: '3'

tasks:
Â  db-deploy:sandbox:
Â  Â  desc: "Deploy to personal sandbox"
Â  Â  cmds:
Â  Â  Â  - databricks bundle deploy -t sandbox

Â  db-deploy:staging:
Â  Â  desc: "Deploy to staging (Internal use/CI)"
Â  Â  cmds:
Â  Â  Â  - databricks bundle deploy -t staging

Â  db-deploy:prod:
Â  Â  desc: "Deploy to production (Service Principal only)"
Â  Â  cmds:
Â  Â  Â  - databricks bundle deploy -t prod --var "sp_id=${PROD_SP_ID}"


```

---

## 7. GitHub Workflows (`.github/workflows/`)

Common files include `lint.yml` (PR validation), `staging.yml` (on push to main), and `production.yml` (on release).

**Example Production Deployment Snippet:**

```yaml
name: Production Deployment
on:
Â  release:
Â  Â  types: [published]
jobs:
Â  deploy:
Â  Â  runs-on: ubuntu-latest
Â  Â  steps:
Â  Â  Â  - uses: actions/checkout@v4
Â  Â  Â  - name: Deploy DAB to Prod
Â  Â  Â  Â  run: databricks bundle deploy -t prod
Â  Â  Â  Â  env:
Â  Â  Â  Â  Â  DATABRICKS_HOST: ${{ secrets.DB_HOST }}
Â  Â  Â  Â  Â  DATABRICKS_TOKEN: ${{ secrets.DB_TOKEN }}
Â  Â  Â  Â  Â  PROD_SP_ID: ${{ secrets.PROD_SP_ID }}


```

---

## 8. Technology Stack & Resource Directory

This section provides a centralized directory of the core technologies, tools, and documentation links that comprise our DataOps platform.

| Component | Technology | Purpose | Documentation |
| --- | --- | --- | --- |
| **Core Framework** | **Databricks Asset Bundles (DABs)** | Declarative infrastructure & resource management. | [DABs Docs](https://docs.databricks.com/en/dev-tools/bundles/index.html) |
| **Task Runner** | **Go Task (Taskfile)** | Command-line abstraction and dev automation. | [Taskfile Guide](https://taskfile.dev/usage/) |
| **CI/CD Platform** | **GitHub Actions** | Orchestration of tests, merges, and deployments. | [GH Actions Docs](https://docs.github.com/en/actions) |
| **Local CI Testing** | **act** | Runs GitHub Actions locally to speed up pipeline dev. | [act Repository](https://github.com/nektos/act) |
| **Version Manager** | **Release Please** | Automates SemVer and Changelog generation. | [Release Please Docs](https://github.com/googleapis/release-please) |
| **Testing Suite** | **pytest** | Python framework for unit and resource validation. | [pytest Docs](https://docs.pytest.org/) |
| **Secret Logic** | **Databricks CLI** | Used for secret-scope creation and API interaction. | [CLI Reference](https://docs.databricks.com/en/dev-tools/cli/index.html) |
| **Communication** | **MS Teams Webhooks** | Automated deployment notifications and ChatOps. | [Teams Webhooks](https://learn.microsoft.com/en-us/microsoftteams/platform/webhooks-and-connectors/how-to/add-incoming-webhook) |
| **Standards** | **Conventional Commits** | Structured commit messages for automation logic. | [Convention Guide](https://www.conventionalcommits.org/) |


---

### Internal Documentation & Helpers

* **Databricks SDK for Python**: Essential for writing custom resource validation scripts in `tests/`.
* [Link: Databricks SDK Reference](https://databricks-sdk-py.readthedocs.io/)

